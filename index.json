[{"content":"","permalink":"https://takumi-iida.com/posts/2024-08-11-ssii_roadmap/","summary":"","title":"SSIIのロードマップ"},{"content":" Project\nPaper\nCode\nOther\nなんだかものすごく久しぶりの投稿ですが、今回はAlbumentationsにおける自作変換クラス作成について書いていきます。\n画像変換するとき、Albumentationsって便利ですよね。なんといっても変換できる種類の数が豊富なのが嬉しいです。\nただ、自分で新たに変換クラスを作成しようとすると、ドキュメントが豊富とは言えないと思うので、誰かの役に立てばと思い、今回の記事を書くことにしました（未来の自分のためかもしれない）。\n実行例はすべてこのリポジトリにあります。\n基本的な使い方 マスク画像を使って画像をクロップする Albumentationsの基本的な使い方を確認するために、以下のコードを実行してみます。画像とマスクの大きさが中途半端で申し訳ないです。\nimage = cv2.imread(\u0026#39;data/images/dog_and_cat.png\u0026#39;) # image.shape: (340, 500, 3) mask = cv2.imread(\u0026#39;data/masks/dog.png\u0026#39;, cv2.IMREAD_GRAYSCALE) # mask.shape: (340, 500) transform = A.Compose([ A.CropNonEmptyMaskIfExists(height=200, width=200, p=1), A.PadIfNeeded(min_height=210, min_width=210, border_mode=cv2.BORDER_CONSTANT, value=(128, 128, 128), mask_value=128, p=1), ]) transformed = transform(image=image, mask=mask_dog) grid_image = make_grid_image(transformed.values(), n_cols=2) # 結果の画像をタイル状に並べる cv2.imwrite(\u0026#39;data/results/crop_dog_by_mask.png\u0026#39;, grid_image) make_grid_image() make_grid_image関数は、複数の画像をタイル状に並べる関数です。\ndef make_grid_image( images: Sequence[np.ndarray], n_cols: int, ): images = [ cv2.cvtColor(image, cv2.COLOR_GRAY2RGB) if image.ndim == 2 else image for image in images] n_rows = len(images) // n_cols h, w, c = images[0].shape grid_image = np.zeros((h * n_rows, w * n_cols, c), dtype=np.uint8) for i, image in enumerate(images): row = i // n_cols col = i % n_cols grid_image[row * h:(row + 1) * h, col * w:(col + 1) * w] = image return grid_image images/dog_and_cat.png masks/dog.png data/results/crop_dog_by_mask.png ここでは、画像(=image)に犬と猫が写った写真。マスク(=mask)には犬の領域に対応したマスク画像を与えています。 データ変換のパイプラインとしては次の処理を行っています。\nA.CropNonEmptyMaskIfExistsでマスクを含むような形で画像を200x200にクロップ A.Resizeで256x256にリサイズ A.PadIfNeededで外周4ピクセル分をを128でパディング ご覧の通り、犬の領域に合わせて画像がクロップされていることがわかります。\nimageとmaskが同様に変換されていることが確認できます。\n解像度の関係でわかりにくいかもしれませんが、imageとmaskによってResizeによる補間処理は異なっています。imageの方は、引数通り線形補間が行われますが、maskの方は最近傍補間が行われています。\nA.Composeに一連の変換処理をリストで渡すことで、変換処理をパイプラインとして実行できます。\n基本的にAlbumentationsの変換処理のフォーマットはNumpy形式と決まっており、A.Composeは、まず初めに入力がそれらの形式になっているのかのチェックやimageとmaskが同じshapeなのかなどのチェックを行います。その後、変換処理を実行していきます。\n今回の記事ではRGB画像とマスク画像しか扱いませんが、他にもBBoxやキーポイントなどもいい感じに変換できます。\nマスクが複数になったら マスクが複数になったら、maskの代わりに各マスクをリストにしたmasksを引数として渡してあげると複数のマスクに同様の変換を適用してくれます。\nimage = cv2.imread(\u0026#39;data/images/dog_and_cat.png\u0026#39;) # image.shape: (340, 500, 3) mask_dog = cv2.imread(\u0026#39;data/masks/dog.png\u0026#39;, cv2.IMREAD_GRAYSCALE) # mask_dog.shape: (340, 500) mask_cat = cv2.imread(\u0026#39;data/masks/cat.png\u0026#39;, cv2.IMREAD_GRAYSCALE) # mask_cat.shape: (340, 500) transformed = transform(image=image, masks=[mask_dog, mask_cat]) grid_image = make_grid_image([transformed[\u0026#34;image\u0026#34;], ] + transformed[\u0026#39;masks\u0026#39;], n_cols=3) cv2.imwrite(\u0026#39;data/results/crop_dog_and_cat_by_masks.png\u0026#39;, grid_image) images/dog_and_cat.png masks/dog.png masks/cat.png data/results/crop_dog_and_cat_by_masks.png A.CropNonEmptyMaskIfExistsは、複数のマスクがあると、それらのマスク領域の和集合を含むようにしてクロップします。この場合、犬か猫の領域のどちらかを少なくとも含むように、領域をクロップします。\nAlbumentationsの各変換モジュールがどのターゲット（mask, bbox, keypoint, global_label）に対して作用するのかは、リファレンスを見るとわかりやすいです。\nPixel-levelの変換は、輝度調整や色調変換、ボケやノイズなどの効果をつけたりするものが多く、画像にのみ作用します。\nSpatial-levelの変換は、クロップやリサイズ、パディングや回転などの幾何変換を伴うものが多く、画像以外にもマスクやBBox、キーポイントなどにも作用します。各クラスによって作用するターゲットが異なるので、リファレンスの表を見て確認する必要があります。\n自作クラスの作成 さて、ここからが本題です。自作の変換クラスを作成してみます。まずは基本的な自作クラスの作成方法を解説します。\n基本編 Albumentationsでは、主に次の3つのクラスをベースクラスとして自作クラスを作成します。\nA.BasicTransform\nA.ImageOnlyTransformとA.DualTransformの基底クラス。\n色々な変換をするときのパラメータを設定するメソッドを持っています。 A.ImageOnlyTransform\n画像のみを変換するときに使う基底クラス。 A.DualTransform\n画像とマスクやBBox、キーポイントを同時に変換するときに使う基底クラス 特殊な変換クラスでもない限り、基本的にはA.ImageOnlyTransformかA.DualTransformを継承して作成していくのが一般的だと思います。\nImageOnlyTransformを継承したクラス A.ImageOnlyTransformを継承して、画像の特定のチャネルだけ残して他を0にする自作変換クラスSelectChannelを作成してみます。\nclass SelectChannel(A.ImageOnlyTransform): def __init__(self, channel: int, always_apply=False, p=1): super(SelectChannel, self).__init__(always_apply, p) self.channel = channel def apply(self, image: np.ndarray, **params) -\u0026gt; np.ndarray: H, W, C = image.shape canvas = np.zeros_like(image) canvas[..., self.channel] = image[..., self.channel] return canvas def get_transform_init_args_names(self): return (\u0026#34;channel\u0026#34;,) 実行例 青だけ残す処理を行ってみます。OpenCVで読み込んでおり、BGRなので0チャネル目が青です。\ntransform = A.Compose([ SelectChannel(channel=0, p=1), ]) image = cv2.imread(\u0026#39;data/images/dog_and_cat.png\u0026#39;) transformed = transform(image=image) cv2.imwrite(\u0026#39;data/results/select_b.png\u0026#39;, transformed[\u0026#34;image\u0026#34;]) images/dog_and_cat.png results/select_b.png このように画像への処理内容はapply()に記述します。\nget_transform_init_args_names()というメソッドは、A.ReplayComposeを使うときに必要になるメソッドです。まだ必要ないのですが、後でA.ReplayComposeの説明もしたいので、まとめて定義しておきます。\napply()には、transform(image=image)で渡されたimageが渡されます。それに対して処理を行って返却すると、返り値の辞書のimageキーに値が格納されます。\nDualTransformを継承したクラス さて、次は指定した領域をクロップするクラス Cropを作成してみます。A.Cropの簡略版です。\nclass Crop(A.DualTransform): def __init__( self, x_min: int, y_min: int, x_max: int, y_max: int, always_apply=False, p=1, ): super(Crop, self).__init__(always_apply, p) self.x_min = x_min self.y_min = y_min self.x_max = x_max self.y_max = y_max def apply(self, image, **params): return image[self.y_min:self.y_max, self.x_min:self.x_max] def apply_to_mask(self, mask: np.ndarray, **params) -\u0026gt; np.ndarray: return mask[self.y_min:self.y_max, self.x_min:self.x_max] def apply_to_masks(self, masks: List[np.ndarray], **params) -\u0026gt; List[np.ndarray]: return [mask[self.y_min:self.y_max, self.x_min:self.x_max] for mask in masks] def get_transform_init_args_names(self): return (\u0026#34;x_min\u0026#34;, \u0026#34;y_min\u0026#34;, \u0026#34;x_max\u0026#34;, \u0026#34;y_max\u0026#34;) 実行例 transform = A.Compose([ Crop(x_min=100, y_min=100, x_max=300, y_max=300, p=1), A.PadIfNeeded(min_height=210, min_width=210, border_mode=cv2.BORDER_CONSTANT, value=(128, 128, 128), mask_value=128, p=1), ]) image = cv2.imread(\u0026#39;data/images/dog_and_cat.png\u0026#39;) mask_dog = cv2.imread(\u0026#39;data/masks/dog.png\u0026#39;, cv2.IMREAD_GRAYSCALE) mask_cat = cv2.imread(\u0026#39;data/masks/cat.png\u0026#39;, cv2.IMREAD_GRAYSCALE) transformed = transform(image=image, masks=[mask_dog, mask_cat]) grid_image = make_grid_image([transformed[\u0026#34;image\u0026#34;],] + transformed[\u0026#39;masks\u0026#39;], n_cols=3) cv2.imwrite(\u0026#39;data/results/crop_constant.png\u0026#39;, grid_image) images/dog_and_cat.png results/crop_constant.png Cropクラスで指定した領域がクロップされていることが確認できます。\nこのクラスは、画像とマスクを同時にクロップする処理を行います。apply()メソッドは画像に対する処理、apply_to_mask()メソッドはマスクに対する処理、apply_to_masks()メソッドは複数のマスクに対する処理を行います。\nA.DualTransformでは、他にも apply_to_bbox()、apply_to_keypoint()、apply_to_global_label() などがあり、それぞれのターゲットに対してメソッドを定義することができます。\nNote 実は、上の変換クラスCropはさらに単純化することができます。apply_to_mask()とapply_to_masksを定義していますが、次のようにapply()メソッドを定義するだけで、maskも同様にクロップされます。imageとmaskは対応したものとして扱われ、同じ変換処理が適用されます。\nそのため、次のCropクラスは上のCropクラスと同じ挙動をします。\nclass Crop(A.DualTransform): def __init__(self, x_min, y_min, x_max, y_max, always_apply=False, p=1): super(Crop, self).__init__(always_apply, p) self.x_min = x_min self.y_min = y_min self.x_max = x_max self.y_max = y_max def apply(self, image, **params): return image[self.y_min:self.y_max, self.x_min:self.x_max] def get_transform_init_args_names(self): return (\u0026#34;x_min\u0026#34;, \u0026#34;y_min\u0026#34;, \u0026#34;x_max\u0026#34;, \u0026#34;y_max\u0026#34;) 唯一違うのはリサイズなどで補間処理を見つけたら、mask（およびmasks）の場合は最近傍補間が適用されるように自動的に修正されるという点です。\nimageとは異なる処理をしたい場合や可読性を上げたい場合を除けば、apply() メソッドのみを定義しておけば十分です。\n応用編 応用編では、より複雑な変換クラスを作成してみます。\nランダム値を扱いたい 適用するごとにランダム値を生成する まずは、ランダムなシフトを行う変換クラスRandomShiftを作成してみます。\nclass RandomShift(A.DualTransform): def __init__( self, x_shift: Tuple[int, int], y_shift: Tuple[int, int], always_apply=False, p=1, ): super().__init__(always_apply, p) self.x_shift = x_shift self.y_shift = y_shift def apply( self, image: np.ndarray, x_shift: int, y_shift: int, **params: Any, ) -\u0026gt; np.ndarray: H, W, *_ = image.shape canvas = np.zeros_like(image) x_min = max(0, x_shift) y_min = max(0, y_shift) x_max = min(W, W + x_shift) y_max = min(H, H + y_shift) canvas[y_min:y_max, x_min:x_max] = image[max(0, -y_shift):min(H, H - y_shift), max(0, -x_shift):min(W, W - x_shift)] return canvas def get_transform_init_args_names(self): return (\u0026#34;x_shift\u0026#34;, \u0026#34;y_shift\u0026#34;) def get_params(self): return { \u0026#34;x_shift\u0026#34;: np.random.randint(*self.x_shift), \u0026#34;y_shift\u0026#34;: np.random.randint(*self.y_shift), } 実行例 上下左右に[-100, 100]ピクセルの範囲でランダムにシフトする処理を行ってみます。\ntransform = RandomShift(x_shift=(-100, 100), y_shift=(-100, 100), p=1) for i in range(2): image = cv2.imread(\u0026#39;data/images/dog_and_cat.png\u0026#39;) mask_dog = cv2.imread(\u0026#39;data/masks/dog.png\u0026#39;, cv2.IMREAD_GRAYSCALE) mask_cat = cv2.imread(\u0026#39;data/masks/cat.png\u0026#39;, cv2.IMREAD_GRAYSCALE) transformed = transform(image=image, masks=[mask_dog, mask_cat]) grid_image = make_grid_image([transformed[\u0026#34;image\u0026#34;],] + transformed[\u0026#39;masks\u0026#39;], n_cols=3) cv2.imwrite(f\u0026#39;data/results/shift_{i:02d}.png\u0026#39;, grid_image) data/results/shift_00.png data/results/shift_01.png ランダムになっていることを確認するために2回実行してみました。2回とも異なるシフトが適用されていることがわかります。さらに、画像とマスクの両方で同じシフトが適用されていることも確認できます。\nget_params()で、適用するごとにランダムな値を生成しています。\nget_params()の返り値は、apply()メソッドの引数に渡されます。今回はapply()の引数にx_shiftとy_shiftを追加して直接受けてみましたが、特に指定しないと**paramsで受け取ることができます。\nコラムでも述べたように、apply()メソッドは他のターゲットにも適用されるので、apply_to_mask()やapply_to_bbox()などでも共有して使うことができます。これにより、画像とマスクが同じシフトを受けることが保証されます。\n入力データに依存したランダム値を生成する 次に、ランダムにクロップする変換クラスRandomCropを作成してみます。RandomCropは、ランダムにクロップする位置を変更しますが、クロップする矩形が画像の範囲を超えないようにするため、画像のサイズに依存したランダム値を生成する必要があります。\nclass RandomCrop(A.DualTransform): def __init__( self, height: int, width: int, always_apply=False, p=1, ): super().__init__(always_apply, p) self.height = height self.width = width def apply( self, image: np.ndarray, x_min: int, y_min: int, x_max: int, y_max: int, **params, ) -\u0026gt; np.ndarray: return image[y_min:y_max, x_min:x_max] def get_transform_init_args_names(self): return (\u0026#34;height\u0026#34;, \u0026#34;width\u0026#34;) def get_params_dependent_on_targets(self, params): image = params[\u0026#34;image\u0026#34;] H, W, C = image.shape x_min = np.random.randint(0, W - self.width) y_min = np.random.randint(0, H - self.height) x_max = x_min + self.width y_max = y_min + self.height return { \u0026#34;x_min\u0026#34;: x_min, \u0026#34;y_min\u0026#34;: y_min, \u0026#34;x_max\u0026#34;: x_max, \u0026#34;y_max\u0026#34;: y_max, } @property def targets_as_params(self): return [\u0026#34;image\u0026#34;] 実行例 画像から200x200サイズの領域をランダムにクロップしてみます。今回もランダムになっているか確認するために2回実行してみます。\ntransform = A.Compose([ RandomCrop(height=200, width=200, p=1), A.PadIfNeeded(min_height=210, min_width=210, border_mode=cv2.BORDER_CONSTANT, value=(128, 128, 128), mask_value=128, p=1), ]) for i in range(2): image = cv2.imread(\u0026#39;data/images/dog_and_cat.png\u0026#39;) mask_dog = cv2.imread(\u0026#39;data/masks/dog.png\u0026#39;, cv2.IMREAD_GRAYSCALE) mask_cat = cv2.imread(\u0026#39;data/masks/cat.png\u0026#39;, cv2.IMREAD_GRAYSCALE) transformed = transform(image=image, masks=[mask_dog, mask_cat]) grid_image = make_grid_image([transformed[\u0026#34;image\u0026#34;],] + transformed[\u0026#39;masks\u0026#39;], n_cols=3) cv2.imwrite(f\u0026#39;data/results/randomcrop_{i:02d}.png\u0026#39;, grid_image) data/results/randomcrop_00.png data/results/randomcrop_01.png 画像内のランダムな位置がクロップされていることが確認できます。クロップ領域が画像の範囲を超えないようになっていることも確認できます。\nget_params_dependent_on_targets()で、paramsに含まれるimageからランダムな値を生成しています。get_params_dependent_on_targets()で利用したいターゲットは、targets_as_paramsプロパティで指定しておかなければなりません。target_as_paramsでparamsのパラメータを収集（辞書を構築）して、get_params_dependent_on_targets()を実行する流れになっています。 get_params()とget_params_dependent_on_targets()を別々に紹介しましたが、get_params_dependent_on_targets()はget_params()の代わりに使うこともできます。両方定義した場合は、互いの返り値の辞書がマージされます。\n他のデータ形式も変換したい Albumentationsの標準では、画像とマスク、BBox、キーポイント、グローバルラベルへの処理をサポートしています。\nしかし、中には文字列やリストなどの他のデータ形式を変換したい場合もあるでしょう。ここでは、入力画像ファイル名の末尾（suffix）に変換した処理名を追加する処理を加えてみましょう。\nclass CropAndAddSuffix(Crop): def apply_to_str(self, string: str, **params) -\u0026gt; str: return string + \u0026#34;/cropped\u0026#34; @property def targets(self) -\u0026gt; List[str]: return { \u0026#34;image\u0026#34;: self.apply, \u0026#34;mask\u0026#34;: self.apply, \u0026#34;image_name\u0026#34;: self.apply_to_str, } 実行例 CropAndAddSuffixを実行しています。\ntransform = CropAndAddSuffix(100, 100, 300, 300, p=1) image_path = Path(\u0026#39;data/images/dog_and_cat.png\u0026#39;) image = cv2.imread(str(image_path)) transformed = transform(image=image, image_name=image_path.stem) print(f\u0026#34;transformed.keys(): {transformed.keys()}\u0026#34;) # ==\u0026gt; transformed.keys(): dict_keys([\u0026#39;image\u0026#39;, \u0026#39;masks\u0026#39;, \u0026#39;image_name\u0026#39;]) print(f\u0026#34;transformed[\u0026#39;image_name\u0026#39;]: {transformed[\u0026#39;image_name\u0026#39;]}\u0026#34;) # ==\u0026gt; transformed[\u0026#39;image_name\u0026#39;]: dog_and_cat/cropped 実行結果を見てみると、image_nameキーに渡したファイル名がdog_and_cat/croppedに変換されていることがわかります。\n前節で作成したCropクラスにapply_to_str()とtargetsプロパティを追加しています。このように targetsプロパティで渡したキーに対して、処理させたいメソッドを指定することで、他のデータ形式にも変換処理を適用できます。\nこうすると、変換時にimage_nameというキーで渡した文字列に対して、apply_to_str()が適用され、/croppedというsuffixを追加します。\nあまりやらないですが、どういう処理をした画像なのかをファイル名に残しておきたい場合などに使えるかもしれません。\n他の入力情報を利用した変換をしたい マスクが複数になったらで紹介したように、複数のマスクがある場合には次のようにmasksにマスクのリストを渡すことで、それぞれのマスクに同じ変換を適用できます。\ntransformed = transform(image=image, masks=[mask_dog, mask_cat]) 例えば、犬と猫のマスクを両方渡して、犬の方のマスク領域だけを抽出する変換クラスExtractDogAreaを素朴に作成すると次のようになります。\nclass CropDogArea(A.DualTransform): def __init__(self, always_apply=False, p=1): super().__init__(always_apply, p) def apply( self, image: np.ndarray, x_min: int, y_min: int, x_max: int, y_max: int, **params, ) -\u0026gt; np.ndarray: return image[y_min:y_max, x_min:x_max] def get_params_dependent_on_targets(self, params): mask = params[\u0026#34;masks\u0026#34;][0] # HERE! indices = np.where(mask \u0026gt; 0) y_min, y_max = indices[0].min(), indices[0].max() x_min, x_max = indices[1].min(), indices[1].max() return { \u0026#34;x_min\u0026#34;: x_min, \u0026#34;y_min\u0026#34;: y_min, \u0026#34;x_max\u0026#34;: x_max, \u0026#34;y_max\u0026#34;: y_max, } @property def targets_as_params(self): return [\u0026#34;masks\u0026#34;] 実行例 transform = A.Compose([ CropDogArea(p=1), A.PadIfNeeded( min_height=200, min_width=130, value=(128, 128, 128), mask_value=128, )], ) image = cv2.imread(\u0026#39;data/images/dog_and_cat.png\u0026#39;) mask_dog = cv2.imread(\u0026#39;data/masks/dog.png\u0026#39;, cv2.IMREAD_GRAYSCALE) mask_cat = cv2.imread(\u0026#39;data/masks/cat.png\u0026#39;, cv2.IMREAD_GRAYSCALE) transformed = transform(image=image, masks=[mask_dog, mask_cat]) grid_image = make_grid_image([transformed[\u0026#34;image\u0026#34;],] + transformed[\u0026#39;masks\u0026#39;], n_cols=3) cv2.imwrite(\u0026#39;data/results/crop_dog_area.png\u0026#39;, grid_image) 左から画像、犬のマスク画像、猫のマスク画像に対応しており、それぞれの犬のマスク領域に対応した領域がクロップされていることがわかります。\ndata/results/crop_dog_area.png しかし、これでは入力したマスクの順序を覚えておかなければならないという問題があります。様々な変換を組み合わせて適用する場合に、それらすべてのクラスで0番目のマスクが犬のマスクであることを共通化するのは難しいですし、利用する際に順序を間違えてしまう可能性もあります。get_params_dependent_on_targets()の中が見苦しいことになっています。\nですので、mask_dogとmask_catをmasksにまとめて渡すのではなく、mask_dogとmask_catをそれぞれdog_maskとcat_maskとして渡せるようにしてみましょう。\nclass CropDogArea(A.DualTransform): def __init__(self, always_apply=False, p=1): super().__init__(always_apply, p) def apply( self, image: np.ndarray, x_min: int, y_min: int, x_max: int, y_max: int, **params, ) -\u0026gt; np.ndarray: return image[y_min:y_max, x_min:x_max] def get_params_dependent_on_targets(self, params): mask = params[\u0026#34;mask_dog\u0026#34;] indices = np.where(mask \u0026gt; 0) y_min, y_max = indices[0].min(), indices[0].max() x_min, x_max = indices[1].min(), indices[1].max() return { \u0026#34;x_min\u0026#34;: x_min, \u0026#34;y_min\u0026#34;: y_min, \u0026#34;x_max\u0026#34;: x_max, \u0026#34;y_max\u0026#34;: y_max, } @property def targets_as_params(self): return [\u0026#34;mask_dog\u0026#34;] get_params_dependent_on_targets()やtargets_as_paramsがmask_dogをそのまま利用できるようになりました。\nしかし、なにも考えずに利用すると、mask_dogとmask_catはマスクとして扱われないため、クロップされません。なので、実行時に「mask_dogとmask_catはマスク処理をする対象のデータである」ことを明示的に指定する必要があります。\nA.Composeのadditional_targets引数を使って、キーにターゲット名、値にターゲットの種類を指定します。\ntransform = A.Compose([ CropDogArea(p=1), A.PadIfNeeded( min_height=210, min_width=140, value=(128, 128, 128), mask_value=128, )], additional_targets={\u0026#34;mask_dog\u0026#34;: \u0026#34;mask\u0026#34;, \u0026#34;mask_cat\u0026#34;: \u0026#34;mask\u0026#34;} ) 実行例 mask_dog, mask_catをそれぞれ処理した結果は同じキーに格納されて返ってきます。\nimage = cv2.imread(\u0026#39;data/images/dog_and_cat.png\u0026#39;) mask_dog = cv2.imread(\u0026#39;data/masks/dog.png\u0026#39;, cv2.IMREAD_GRAYSCALE) mask_cat = cv2.imread(\u0026#39;data/masks/cat.png\u0026#39;, cv2.IMREAD_GRAYSCALE) transformed = transform(image=image, mask_dog=mask_dog, mask_cat=mask_cat) grid_image = make_grid_image([ transformed[\u0026#34;image\u0026#34;], transformed[\u0026#34;mask_dog\u0026#34;], transformed[\u0026#34;mask_cat\u0026#34;]], n_cols=3) cv2.imwrite(\u0026#39;data/results/crop_dog_area.png\u0026#39;, grid_image) 処理結果の画像は上の結果と同じですが再掲します。\n左から画像、犬のマスク画像、猫のマスク画像に対応しており、それぞれの犬のマスク領域に対応した領域がクロップされていることがわかります。\ndata/results/crop_dog_area.png additional_targets \u0026ndash;\u0026gt; targetsのように辞書をたどって行くことで、処理対象のメソッドへとたどり着きます。\nA.DualTransformには次のtargetsが定義されているので、\u0026quot;mask_dog\u0026quot; \u0026ndash;\u0026gt; \u0026quot;mask\u0026quot; \u0026ndash;\u0026gt; apply_to_mask() にたどり着きます。\n@property def targets(self) -\u0026gt; Dict[str, Callable[..., Any]]: return { \u0026#34;image\u0026#34;: self.apply, \u0026#34;mask\u0026#34;: self.apply_to_mask, \u0026#34;masks\u0026#34;: self.apply_to_masks, \u0026#34;bboxes\u0026#34;: self.apply_to_bboxes, \u0026#34;keypoints\u0026#34;: self.apply_to_keypoints, } 複数種類の入力データを組み合わせた処理をしたい \u0026amp;\u0026amp; 返り値を追加したい これまでの変換処理は、それぞれの画像やマスクに対して処理を行い、その結果をimageやmasksとして返していました。しかし、処理結果に追加の情報を返したい場合や、複数種類の入力データを組み合わせた処理を行いたい場合もあるでしょう。\nこの節では、複数の別々に用意したカテゴリマスクを一枚の画像にまとめる処理を行う変換クラスMergeMasksを作成してみます。\n具体的には、犬、猫、芝生、木の4つのマスクを別々に用意したので、それらを一枚の画像にまとめてセグメンテーション用のマスクを作成します。ただし、犬、猫はオブジェクトクラス、芝生、木は背景クラスとしてラベルをまとめます。\nclass MergeMasks(A.DualTransform): def __init__( self, object_value: Tuple[int, int, int] = (255, 0, 0), bg_value: Tuple[int, int, int] = (0, 255, 0), always_apply=False, p=1, ): super().__init__(always_apply, p) self.object_value = object_value self.bg_value = bg_value def apply(self, img: np.ndarray, **params) -\u0026gt; np.ndarray: return img def apply_with_params( self, params: Dict[str, Any], *args: Any, **kwargs: Any, ) -\u0026gt; Dict[str, Any]: res = super().apply_with_params(params, *args, **kwargs) H, W = res[\u0026#34;mask_dog\u0026#34;].shape canvas = np.zeros((H, W, 3), dtype=np.uint8) mask_object = np.logical_or(res[\u0026#34;mask_dog\u0026#34;], res[\u0026#34;mask_cat\u0026#34;]) mask_bg = np.logical_or(res[\u0026#34;mask_grass\u0026#34;], res[\u0026#34;mask_tree\u0026#34;]) canvas[mask_object] = self.object_value canvas[mask_bg] = self.bg_value res[\u0026#34;mask_merged\u0026#34;] = canvas res[\u0026#34;image_overlay\u0026#34;] = cv2.addWeighted(res[\u0026#34;image\u0026#34;], 0.5, canvas, 0.5, 0) return res def get_transform_init_args_names(self) -\u0026gt; Tuple[str]: return (\u0026#34;object_value\u0026#34;, \u0026#34;bg_value\u0026#34;) 実行例 画像と各マスクを[-10, 10]度の範囲でランダムに回転させ、MergeMasksでマスクをマージしてみます。\n処理の対象がわかりやすいように、少しコードが長くなってしまいますが、犬、猫、芝生、木のマスクを読み込んでtransformに渡しています。\n返り値には、マージ後のマスクmask_mergedと、画像とマスクを重ねた画像image_overlayが追加されているので、それらを描画してみました。\ntransform = A.Compose([ A.Rotate(limit=10, border_mode=0, p=1), MergeMasks(object_value=(0, 170, 246), bg_value=(255, 90, 0), p=1), ], additional_targets={ \u0026#34;mask_dog\u0026#34;: \u0026#34;mask\u0026#34;, \u0026#34;mask_cat\u0026#34;: \u0026#34;mask\u0026#34;, \u0026#34;mask_grass\u0026#34;: \u0026#34;mask\u0026#34;, \u0026#34;mask_tree\u0026#34;: \u0026#34;mask\u0026#34;, }) image = cv2.imread(\u0026#39;data/images/dog_and_cat.png\u0026#39;) mask_dog = cv2.imread(\u0026#39;data/masks/dog.png\u0026#39;, cv2.IMREAD_GRAYSCALE) mask_cat = cv2.imread(\u0026#39;data/masks/cat.png\u0026#39;, cv2.IMREAD_GRAYSCALE) mask_grass = cv2.imread(\u0026#39;data/masks/grass.png\u0026#39;, cv2.IMREAD_GRAYSCALE) mask_tree = cv2.imread(\u0026#39;data/masks/tree.png\u0026#39;, cv2.IMREAD_GRAYSCALE) transformed = transform( image=image, mask_dog=mask_dog, mask_cat=mask_cat, mask_grass=mask_grass, mask_tree=mask_tree, ) grid_image = make_grid_image([ transformed[\u0026#34;image\u0026#34;], transformed[\u0026#34;mask_merged\u0026#34;], transformed[\u0026#34;image_overlay\u0026#34;]], n_cols=3) cv2.imwrite(\u0026#39;data/results/merged_mask.png\u0026#39;, grid_image) data/results/merged_mask.png 複数のマスクを組み合わせたり、追加の情報を返す場合は、apply_with_params()メソッドを使うと実現できます。\n本来のapply_with_params()は、各ターゲットに対して適用するメソッドの選択やちょっとしたパラメータの更新を行い、実際に各処理（apply()やapply_to_mask()など）を実行するメソッドです。 言い換えると、各入力情報を保持しており、apply_with_params()の返り値がtransformしたときの返り値となります。ですので、ここでは返り値の辞書に新たな要素を追加しています。 引数であるparamsには、get_params()とget_params_dependent_on_targets()から受け取った値が入っており、args, kwargsには、各入力データが入っています。\nベースクラスのリファレンス メソッド or プロパティ 説明 apply() 画像に対する変換処理を行うメソッド。 A.DualTransform の場合は、マスクに対する処理も行う。 targets_as_params get_params_dependent_on_targets()で利用するターゲットを指定 add_targets() 辞書形式で追加で処理したいターゲット名とその対象として扱わせたいターゲット名を指定します。上の例で出していたA.Composeのadditional_targetsと同じです。A.Composeでadditional_targetsを指定すると、内部的には各変換クラスのadd_targets()が呼ばれています。 apply_with_params() targetsで指定した辞書に従って、各ターゲットに対する処理を行う。処理を行う前にupdate_params()によるパラメータ更新も行う。 get_params() 入力データに依存しないパラメータを準備する関数。変換ごとに一度しか呼ばれないので、同時に入力したデータでランダム値を共有したい場合に便利。 get_params_dependent_on_targets() 入力データに依存したパラメータを準備する関数。get_params()の上位互換という印象。 get_transform_init_args_names() そのクラスを初期化するのに必要なパラメータ名のリストを返す。通常は不要だが、A.ReplanComposeなどで同じパラメータを使いまわす場合に使う。 set_deterministic() targets 変換対象のターゲット名とそれを処理するメソッドを指定する。 update_params() apply_with_params()内部で最初に呼ばれる。 paramsとkwargsの両方が渡されるので、get_params(), get_params_dependent_on_targets()の中で一番使える情報が多い。 変換に利用されるパラメータparamsは、次の3つのメソッドを順番に呼び出し、辞書を更新していきます。\nget_params() get_params_dependent_on_targets() update_params() ","permalink":"https://takumi-iida.com/posts/2024-06-14-albumentations/","summary":"Project Paper Code Other なんだかものすごく久しぶりの投稿ですが、今回はAlbumentationsにおける自作変換クラス作成について書いていきます。 画像変換","title":"Albumentationsにおける自作変換クラス作成"},{"content":" @Article{kerbl3Dgaussians, author = {Kerbl, Bernhard and Kopanas, Georgios and Leimkhler, Thomas and Drettakis, George}, title = {3D Gaussian Splatting for Real-Time Radiance Field Rendering}, journal = {ACM Transactions on Graphics}, number = {4}, volume = {42}, month = {July}, year = {2023}, url = {https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/} } Project\nCode\n高解像度(1080p)で学習時間が短く、描画時間もリアルタイム（\u0026gt;=30fps）であるようなレンダリング手法を提案する。\nカメラキャリブレーションから生成されるスパースな点から3Dガウシアンを使ってシーンを表現する。この3Dガウシアンはシーンを最適化する連続的なボリューム輝度場を持ちながら、空のスペースに対しては不要な計算を行わないという理想的な性質を持つ。 3Dガウシアンのインタリーブ最適化と密度制御を行い、異方性共分散を最適化することでシーンを正確に表現する。 Visibility-Awareな高速レンダリングを行う。異方性スプラフティングをサポートし、学習の高速化＆リアルタイムレンダリングを可能にする。 比較 NeRFやその派生系(voxel, hash grids, points)のVolumetric Ray-Marchingを使った方法は連続値を扱えるので最適化しやすいが、サンプリングに時間がかかったり、ノイジーになりやすいデメリットがある。\nこの論文のゴール\n複数のシーン画像からのリアルタイムレンダリング 既存手法よりも数倍速い最適化 3D Gaussian Splatting 全体図 SfMのスパースな点群を出発点として、高解像度な新規視点のシーンを最適化する必要があり、それには微分可能なボリュームレンダリングの性質を持ちつつ、高速なレンダリングができるプリミティブが必要。 3D Gaussianはそういった性質を持つし、アルファブレンディングできる2D上に投影することができる。\n3D Gaussianに近い論文＝小さな法線をもつ円盤を使う\nしかし、これは法線を推定するのが困難で、それを最適化するのも難しい。\n3D Gaussianは世界座標系で中心点（平均） $\\mu$ の3次元共分散行列 $\\Sigma$ で定義される。\n$$ G(x)=e^{-\\frac{1}{2}(x)^T \\Sigma^{-1}(x)} $$\nこのガウシアンは$\\alpha$を掛けることでブレンディングできる。\n3Dから2Dにレンダリングするときは、次式で行える。\n$$ \\Sigma^\\prime = JW\\Sigma W^TJ^T $$\nここで、J＝透視投影変換のアフィン近似のヤコビアン、W＝視点変換行列、$\\Sigma^\\prime$＝カメラ座標系の共分散行列。\nこの3D Gaussianの3次元共分散行列 $\\Sigma$を直接最適化できれば良いが、共分散行列は半正定値のときのみ物理的な意味を持つのでうまく行かない。制約を設けて勾配降下法をしてもうまく行かない。 純粋な共分散行列を最適化しようとしてもだめなので、3D Gaussianを楕円体とみて共分散行列っぽいものを作ってみる。\n$$ \\Sigma = RSS^TR^T $$\nここで、R＝回転行列、S＝スケール行列。このRとSを両方最適化したいがうまくいかないので、別々に最適化する。 （自動微分のオーバーヘッドを避けるために、明示的に手計算で微分している模様\u0026hellip;）\n最適化後の3Dガウシアン（右）。異方性のある形状を表現できている。 3D Gaussianの適応的密度制御による最適化 最適化 3D Gaussianの\n密度\n+ 位置\n+ 透明度 $\\alpha$\n+ 共分散行列 $\\Sigma$\n+ 球面調和係数（各Gaussianの色）\nの最適化を行う。\n反復的なレンダリングで生成された画像と学習画像とを比較して最適化を行う。しかし、3D-\u0026gt;2Dの投影は曖昧さを含み不確実性が伴う。 そのため、まず幾何情報を作成して、間違っていたら削除したり動かしたりできるような最適化を行う。\nGPUアクセラレートフレームワークやカスタムCUDAカーネルを追加して高速化を図っている。後述の高速なラスタライズも重要。\n$\\alpha$に対してはsigmoid関数を適用して[0, 1)になるようにしたり、勾配がなめらかになるようにしている。また、指数関数的な活性化関数は共分散行列のスケールするのに使っている。\n初期の共分散行列は近傍の3点への距離平均に等しい軸を持つ等方性のガウシアン。 Plenoxelsと同様に指数的減衰スケジューラを使用するが、「位置」のみに適用。 損失関数としては次式になる。これを確率的勾配降下法(SGD)で最適化。\n$$ \\mathcal{L}=(1-\\lambda) \\mathcal{L}_1+\\lambda \\mathcal{L}_S $$\nここで、$\\lambda=0.2$\n適応的なガウシアンの制御 SfMのスパースな点群を初期位置として、適応的にガウシアンの数や単位ボリューム（ガウシアンではなく、その場所の密度という意味だと思う）の密度を制御する。\nWarm-up最適化をしたあと、100イテレーションごとに密にして、$\\alpha$がしきい値$\\epsilon_{\\alpha}$を下回ったら削除する。\nガウシアンの適応的な制御は空のスペースに配置する必要がある。 これは、幾何学的な特徴が欠落している領域に焦点を当てるだけではなく、ガウシアンがカバーするシーンの大きな領域の両方に焦点を当てている。（意味がよくわからない）\n$\\tau_{pos}=0.0002$以上の位置勾配の大きさをもつガウシアンを高密度化。（上図）小さいガウシアンはクローンして位置勾配の方向に従って移動。（下図）大きな分散を持つ大きいガウシアンは小さく分割。 最適化をしていくと入力カメラ付近にfloater（ホコリみたいなやつ）が積層していくので、N=3000イテレーションごとに$\\alpha$を0に近づける。 他にも大きすぎるガウシアンは排除する。\n高速な微分可能なガウシアンのラスタライズ 目標＝高速なレンダリング＋高速なソート\n$\\rightarrow$ $\\alpha$ブレンディングをやったり、スプラットの回数制限を避けるため\n$\\rightarrow$タイルベースのラスタライズ＝プリミティブを画像全体に対して事前にソートして、ピクセルごとにソートを行う計算量を避ける ref\nこのラスタライズは任意の数の混合ガウシアンに対して効率的なバックプロパゲーションを行え、メモリ消費量が少ない。（ピクセルごとのオーバーヘッドが必要なだけ）\n画面を$16 \\times 16$のタイルに分割して、ビューの台形（frustum）に応じて3Dガウシアンをタイルごとに間引く。信頼区間99%のガウシアンだけ残す。 さらに、ガードバンドを設けてFrustumから近すぎたり、遠すぎたりするものを棄却する。（投影される二次元共分散の計算が不安定になりがちだから） タイルの重なりに応じてガウシアンをインスタンス化、その後深度とタイルIDを組み合わせてインスタンスにキーを割り当てる。 ガウシアンをキーに応じてソート（single fast GPU Radix sortを使う） このソートに基づいて$\\alpha$ブレンディングが行われる。\nはじめの方は$\\alpha$ブレンディングはいくつかの構成の近似として動作するが、スプラットがピクセルサイズに近づくにつれてこの近似は無視できるようになっていく。\nタイルにスプラットされ、深度でソートされたリストを生成し、タイルごとにラスタライズするためのスレッドブロックを起動しておく。各スレッドブロックは共有メモリにガウシアンパケットをロードして、各ピクセルに色と$\\alpha$をfront-to-back方向に累積していく。$\\alpha$値が飽和したらそのスレッドを停止させる。定期的にタイルのスレッドをチェックして、全ピクセルが飽和していたらスレッド全てを停止する。\n先行研究と異なり、勾配を受け取るプリミティブの数を制限してない。（シーン固有のハイパーパラメータが不要という意味）\n結果 Mildenhallの研究で使われた13つのリアルなシーンで実験。\nインドア・アウトドアデータで実験。Mip-NeRF360は品質、InstantNGPとPlenoxelsは速さ比較用途。8枚目をテスト用として使用。 定量評価 イテレーション数の違い Mip-NeRF360は48hかかるが、3D Gaussian Splattingは35-45minで学習できる。 InstantNGPやPlenoxelsは5-10minで終わるが、品質は劣る。\n合成データ(Blender)での結果。正確なカメラパラメータが得られる。ランダムな初期値でもかなり良い結果になる。 Ablations SfMの初期値を使うことの重要性。ランダムでも全体的には良い結果が得られるが、背景部分にはモヤ（floaters）が出現する。 ガウシアンの分割やクローンをやらない場合などの違い。背景を良くするには大きなガウシアンの分割が必要、一方小さなガウシアンのクローンは高品質で高速な収束に必要。 3Dガウシアンが異方性（楕円体）を持つ必要性の確認。等方性である（球体）だと明らかに低品質 Limitations 観測シーンが少ない場合はアーチファクトが発生 細長いアーチファクトやポツポツとした3Dガウシアンを作成する可能性がある 大きなガウシアンが作られた場合にポッピングアーチファクト（LODの切り替えタイミングで起こるアーチファクト）が時々発生 ポッピングのイメージ 視点位置依存のアピアランスの影響で発生するのでは ラスタライザのガードバンドにより棄却されてしまうのでは 単純な実装だと20GBくらい使うが、低レベル実装したら学習に数百MB、ラスタライズには追加で30-500MBくらいで済む（解像度依存）。 ","permalink":"https://takumi-iida.com/posts/2023-11-14-3d-gaussian-splatting/","summary":"@Article{kerbl3Dgaussians, author = {Kerbl, Bernhard and Kopanas, Georgios and Leimkhler, Thomas and Drettakis, George}, title = {3D Gaussian Splatting for Real-Time Radiance Field Rendering}, journal = {ACM Transactions on Graphics}, number = {4}, volume = {42}, month = {July}, year = {2023}, url = {https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/} } Project Code 高解像度(1080p)で学習時間が短く、描画","title":"3D Gaussian Splatting"},{"content":" Project\n2. TensorRT\u0026rsquo;s Capabilities TensorRTには、モデルの定義とターゲットGPUへの最適化を行うビルドフェーズと、最適化されたモデルを実行する実行フェーズの2つのフェーズがあります。\n2.1 Build Phase Builder を使って、モデルの最適化や Engineの作成を行う。手順は次の通り。\nネットワークを定義する NetworkDefinitionインタフェースを定義する。2通りの方法がある。 ONNXからTensorRTのONNX parserを呼び出す方法 TensorRTのLayerやTensorを直接呼び出してネットワークを定義する方法の2つがある。 注意：出力としてマークされていないTensorは一時テンソルとして破棄されるので、出力したかったら名前を指定してやる必要がある。 ネットワークのconfigを指定する BuilderConfigインタフェースでTensorRTがどうやってモデルを最適化するかを指定する。 精度 (Precision)。 実行時スピードとメモリ効率とのトレードオフの制御 CUDAカーネルの選択の制約 Builderを呼び出してEngineを作る 1., 2.の情報を使って、Engineインタフェースを作る。 TensorRTのバージョンとターゲットGPUの種類によってエンジンが作成される。 TensorRTのネットワーク定義は浅いコピーなので、ビルドフェーズでメモリの開放はやらないで ビルダは一つだけ動かす。 ビルダは最速時間を計測するが、他のGPUでビルダが動いていると実行タイミングがずれるので最適化が弱くなる 2.2 Runtime Phase 実行のさせかた。最高位APIは Runtime クラス。Runtimeを使った実行のさせかたは次の通り。\nTensorRTエンジンをデシリアライズ エンジンから実行コンテクストを作成 そのとき、入出力バッファを用意する必要がある。 推論自体は enqueueV3をコールすれば実行できる。\nEngineインタフェース最適化済みモデルを持っており、はネットワークの入出力情報を提供できる。 一方で、そのEngineから作成されたExecutionContextは推論を呼び出すインタフェース。一つのエンジンに関連付けられた複数の実行コンテクストを作成して、並列実行できる。\n入出力のバッファをCPUかGPU上に用意するが、エンジンに問い合わせてどちらにバッファを用意するかを決定できる。 バッファを用意したらenqueueV3で実行できる。これにより、必要なカーネルがCUDAストリームにエンキューされ、すぐにアプリケーションの制御が戻される。 CPUとGPUの転送で時間がかかるが、こういった非同期処理を待機したい場合は cudaStreamSynchronizeを使ってストリームを同期する。\n2.3 Plugins TensorRTだけでは対応していないオペレーションの実装を提供する機構。TensorRTのPluginRegistryに登録することで、モデル変換時にONNXパーサがプラグインを利用できるようになる。 詳細\n2.4 Types and Precision TensorRTはFP32, FP16, INT8, INT32, UINT8, BOOLのデータ型に対応している。\nFP32, FP16 非量子化 INT8 暗黙的量子化 スケールファクタ(dynamic_ranges)が必要。（キャリブレーションかsetDynamicRange APIで指定） 明示的量子化 符号付き整数に解釈する。Q/DQレイヤを明示的につかってINT8型に相互変換する。 UINT8 入出力タイプにだけ利用できるデータ型 入力はUINT8からFP32 or FP16に変換される（CastLayer） 出力もCastLayerでUINT8を出力する。 量子化は非対応 ConstantLayerは出力タイプとしてはUINT8に非対応 BOOL Precisionを指定する方法は次の2つある。\nモデルレベル：BuilderFlagオプションで低精度を指定する レイヤレベル：レイヤごとに精度を指定して、数値的にセンシティブな箇所に対処する 2.5 Quantization Dynamic rangeはビルダ（キャリブレーション）かQATで計算できる。\nTODO: ここはもう少し詳しく書く\nTensorRTの量子化はSymmetric Uniform Quantization（Siggned INT8）。\n量子化前後の変換は単純な乗算で表現できる。\n量子化対象：アクティベーション、重み\nアクティベーション向けの量子化は、キャリブレーションアルゴリズムに依存する。 重み向けの量子化は、\n$$ s=\\frac{\\max \\left(\\operatorname{abs}\\left(x_{\\min }\\right), \\operatorname{abs}\\left(x_{\\max }\\right)\\right)}{127} $$\nで計算される。\n量子化\nこのスケールsが与えられたとき、量子化/逆量子化演算は $ x_q=[-128, 127] $ の整数値、$ x $をアクティベーションの浮動小数点とすると、\n$$ x_q=\\text { quantize }(x, s):=\\operatorname{roundWithTiesToEven}\\left(\\operatorname{clip}\\left(\\frac{x}{s},-128,127\\right)\\right) $$\nroundWithTiesToEvenは、最も近い偶数になる。23.5や24.5は24、-23.5や-24.5は-24になる。\nただし、OrinのDLA向けだとちょっと丸め関数が違うらしい\n$$ x_q=\\text { quantize }(x, s)=\\text { roundWithTiesToNearestEven }\\left(\\operatorname{clip}\\left(\\frac{x}{s},-128,127\\right)\\right) $$\n丸め関数 逆量子化\n$$ x=\\operatorname{dequantize}\\left(x_q, s\\right)=x_q * s $$\n量子化演算を有効にするには Builder config でINT8フラグを有効にする必要がある。\n暗黙的量子化 各量子化テンソルに紐づいたスケールを使って暗黙的な量子化や逆量子化を行う。\n暗黙的量子化の場合、TensorRTはまずグラフを最適化するときには浮動小数点モデルとして扱い、レイヤがINT8で高速になる場合にINT8で実行する。それ以外はFP32かFP16。\nAPIレベルでレイヤごとに明示的に精度を設定しても、TensorRTのグラフ最適化中に別のレイヤと融合することがあるのでレイヤごとの精度の情報が失われることがある。\nINT8が使われるかどうかが制御しづらい。\n明示的量子化 スケーリング演算を使って量子化、逆量子化が明示的に iQuantizeLayerとIDeqantizeLayerノード（Q/DQノード）によって行われる。\n明示的量子化ではINT8で量子化することを明示的に指定できる。\nImplicit vs Explicit Quantization Q/DQレイヤのあるネットワークをINT8でビルドするときはフラグを立てる必要がある。\nconfig-\u0026gt;setFlag(BuilderFlag::kINT8); 明示的量子化では、ネットワークはINT8に双方向に型変換を行うので、INT8を型成約としてはいけない。\n重み\nQ/DQモデルの重みはFP32で指定される。IQuantizeLayerのスケールを使ってTensorRTが重みを量子化する。量子化された重みはEngineファイルに格納される。\nONNX\nPyTorchやTensorFlowからエクスポートされるONNXにはQ/DQノード（Qノードの後にDQノードが続く、Fake-Quantization）が明示的に使われることがある。\nTensorRTではそれらのQ/DQレイヤのセマンティクスを保持するので、性能劣化が少ない。（意訳）\nしかし、内部の浮動小数点演算の順序が変わる可能性があるので、ビット単位で結果が一致することはない。\nONNXのopset=10からQuantizeLinearとDequantizeLinearが追加され、TensorRTはこれをIQuantizeLayerとIDequantizeLayerにマッピングする。\nopset=13(PyTorch=1.8以降)では量子化する軸が追加され、チャネルごとの量子化ができるようになった。\n注意：ONNXのGEMM演算はチャネルごとに量子化できる。PyTorchの torch.nn.LinearレイヤはONNXでは(K, C)の重みとtransB属性（GEMM演算をする前に重みの転置を行う）を持つGEMM演算に置き換えられる。TensorFlowでは事前に転置済みの(C, K)のGEMMになる。$K=出力チャネル数, C=入力チャネル数$\nPyTorch: $ y = xW^T $ ONNX: $ y = xW $ PyTorchの重みはTensorRTで転置されるので、その重みは転置前にTensorRTによって量子化される。そのため、PyTorchからエクスポートされるONNX QATモデルは0次元目（$K=0$）でチャネルごとの量子化を行う。一方で、TensorFlowだと1次元目（$K=1$）でチャネルごとの量子化を行う。\nTensorRTは量子化済みオペレータをサポートしていない。\nつまり、ONNXの量子化済みオペレータ\nQLinearConv/ QLinearMatmul ConvInteger / MatmulInteger に遭遇したらインポートエラーを吐く。 量子化スケール 次の2種類の粒度でスケーリングできる。\nテンソルごとのスケール：単一のスケール値でテンソル全体をスケーリング チャネルごとのスケール：指定された軸にそってスケール値をブロードキャストしてスケーリング 重みはどちらかの方法でスケール、アクティベーションはテンソルごとのスケーリングのみ。\n例）重みのチャネルごとのスケーリング。2D Convカーネルの重みのshapeが KCRSでKが出力チャネル数だとすると、\n出力チャネルに対してスケーリングすることに注意する。ただし、Deconvolutionは入力チャネルに対してスケーリングする。\nfor k in range(K): for c in range(C): for r in range(R): for s in range(S): weight[k, c, r, s] = clamp(round(weight[k, c, r, s] / scale[k]), -128, 127) ↑の例で逆量子化の場合は\nfor k in range(K): for c in range(C): for r in range(R): for s in range(S): output[k, c, r, s] = input[k, c, r, s] * scale[k] Dynamic Range Dynamic rangeは量子化されたテンソルによってカバーされる範囲で、外部で求められた暗黙的な量子化に使われる。 dynamic rangeは(min, max)が設定できるが、TensorRTはSymmetric Uniform Quantizationしかサポートしていないので、 max(abs(min_float), abs(max_float)) でスケールされる（大きい方）。\nPost-Training Quantization Using Calibration (PTQ) 代表的な入力データを使って、モデル中のアクティベーションの統計情報を計算し、ベストなスケール値を求める。\n入力データは500画像くらいあれば良い。\n量子化誤差：\n離散化誤差（レンジが増えると増大する） 切り捨て誤差（レンジにより切り捨てられる値） とのバランスでスケールは求める。そのため、TensorRTにはキャリブレータがいくつかある。\nkCALIBRATE_BEFORE_FUSION: キャリブレーション前に性能に影響のないレイヤフュージョンを行うキャリブレータ。しかし、DLAを使う場合に問題ある。\nキャリブレーションのバッチサイズは、IInt8EntropyCalibrator2 と IInt8EntropyCalibrator の切り捨て誤差に影響がでる。\n小さなキャリブレーションバッチはヒストグラムの解像度が低下し、スケールの精度が低下する。アクティベーションの値がヒストグラムの最大値よりも高いと、ヒストグラムの範囲は2のべき乗ごとに増える。\n最終のキャリブレーションステップで再割り当てがおきてヒストグラムのビンの半分（片側）が空になる場合以外は良い結果になる。キャリブレーションバッチの順序に影響を受ける。（最後出なかったら切り捨てられることもあるから？）\nなので、できるだけバッチサイズを大きくしたほうが良い。\nキャリブレータ\nIInt8EntropyCalibrator2: 量子化後の情報理論に基づいたエントロピーキャリブレーション。外れ値は除去される。DLAに必要。デフォルトでは、キャリブレーションはレイヤフュージョン前に行われる。CNN向け。 IInt8MinMaxCalibrator: アクティベーションのフルレンジを利用。デフォルトでは、キャリブレーションはレイヤフュージョン前に行われる。NLP向け。BERT向け。 IInt8EntropyCalibrator: LegacyCalibratorよりもシンプルで良い結果が出る。デフォルトでは、キャリブレーションはレイヤフュージョン後に行われる。 IInt8LegacyCalibrator: TensorRT 2.0 EAと互換性のあるキャリブレーション。ユーザのパラメータ化や必要で悪い結果の場合フォールバックされる。デフォルトでは、キャリブレーションはレイヤフュージョン後に行われる。パーセンタイルでキャリブレーションできる (99.99%パーセンタイル)。BERTやNeMo ASR model QuartzNetなどで良い結果だった（昔は？）。 手順としては、\n32ビットエンジンをビルドして、アクティベーションのヒストグラムを作ってキャリブレーションする。 ヒストグラムをもとに各テンソルのキャリブレーションテーブル（スケール値）を作る。 INT8エンジンをキャリブレーションテーブルをもとにビルドする。 キャリブレーションは遅いので、ステップ2のテーブルは保存しておいた方が良い。\nレイヤフュージョン前のキャリブレーションだったらデバイス間で移植可能。\nつまり、IInt8EntropyCalibrator2やIInt8MinMaxCalibratorを使うときやQuantizationFlag::kCALIBRATE_BEFORE_FUSIONフラグがセットされているときは移植できる。 レイヤフュージョンはプラットフォームやデバイスによって動作が異なることがあるので、フュージョン後だとキャリブレーションキャッシュが利用できないかもしれない。\nNote: ビルダがINT8の入出力を使うと設定されていても、TensorRTはキャリブレーションデータはFP32であることを想定している（入出力はFP32）。このときはINT8のI/OをFP32にキャスト（[128.0F, 127.0F]の範囲）する必要がある。\nNote: キャリブレーションは決定的で、TensorRTに同じデータ、同じ順序、同じデバイスで入力されたら、同じスケール値が出る。\n2.7 Dynamic shape TensorRTは入力形状に基づいてモデルを最適化するが、実行時に動的な形状をサポートしている。OptimizationProfile最小、最大入力形状を指定する。\nTODO: ここはもう少し詳しく書く\n2.8 DLA TODO: ここはもう少し詳しく書く\n2.10 trtexec ランダム値 or ユーザ指定のデータを使ったネットワークベンチマーク モデルのエンジン化（シリアライズ化） ビルダからシリアライズ化されたタイミングキャッシュを作成 2.11 Polygraphy TensorRTモデルの実行やデバッグをするためのツール。\n複数のバックエンドで実行する (TensorRT, ONNX-runtime) モデルを複数フォーマットに変換する。 e.x. TensorRT engine with post-training quantization モデルの様々なタイプの情報表示 ONNXモデルをコマンドラインから変更 サブグラフの抽出 単純化やサニタイズ化 (simplify and sanitize) I/O Formats TODO: ここはもう少し詳しく書く\n","permalink":"https://takumi-iida.com/posts/2023-11-13-tensorrt_docs/","summary":"Project 2. TensorRT\u0026rsquo;s Capabilities TensorRTには、モデルの定義とターゲットGPUへの最適化を行うビルドフェーズと、最適化されたモデルを実行する実行フェーズの2つ","title":"TensorRT Developer Guideを読んだメモ"},{"content":" @InProceedings{pmlr-v202-salman23a, title = {Raising the Cost of Malicious AI-Powered Image Editing}, author = {Salman, Hadi and Khaddaj, Alaa and Leclerc, Guillaume and Ilyas, Andrew and Madry, Aleksander}, booktitle = {Proceedings of the 40th International Conference on Machine Learning}, pages = {29894--29918}, year = {2023}, editor = {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan}, volume = {202}, series = {Proceedings of Machine Learning Research}, month = {Jul}, publisher = {PMLR}, pdf = {https://proceedings.mlr.press/v202/salman23a/salman23a.pdf}, url = {https://proceedings.mlr.press/v202/salman23a.html}, abstract = {We present an approach to mitigating the risks of malicious image editing posed by large diffusion models. The key idea is to immunize images so as to make them resistant to manipulation by these models. This immunization relies on injection of imperceptible adversarial perturbations designed to disrupt the operation of the targeted diffusion models, forcing them to generate unrealistic images. We provide two methods for crafting such perturbations, and then demonstrate their efficacy. Finally, we discuss a policy component necessary to make our approach fully effective and practical—one that involves the organizations developing diffusion models, rather than individual users, to implement (and support) the immunization process.} } Project\nPaper\nCode\n悪意あるユーザによる画像編集を防ぐPhotoGuardを提案。フェイクニュースを防ぐことが目的。\n摂動を加えて、AIによる画像編集コストを上げる。（この論文では予防接種と呼んでいる）\nPhotoGuard 攻撃方法（予防方法）は次の2種類。\nEncoder Attack Diffusion Attack Encoder Attack LDMのImage Encoder $\\mathbf{\\epsilon}$ に対してPGD攻撃する。\n\u0026ndash;\u0026gt; 悪いRepresentationを生成する。\nアーティファクトは発生しているが、目的の男性は生成できてしまっている。\nテキストを無視できていない。\n中間表現を攻撃しているだけ。生成結果の保証がない。\n\u0026ndash;\u0026gt; Diffusion Attack\nDiffusion Attack 最終的に生成された画像が失敗するようなPGD攻撃を行う。\n結果 対象モデル＝Stable Diffusion v1.5 目標＝無関係な画像の生成 or 非写実的な画像の生成\nEncoder Attackした結果。対策画像は非写実的な画像になっている。 画像編集で実験した場合の結果。「二人の頭部」は編集しないようなバイナリマスクが与えられている。 60の異なるテキストで編集した場合の結果。（今回は攻撃なので）値が悪いほうが良い。元画像の類似性。 プロンプトとの類似性 ","permalink":"https://takumi-iida.com/posts/2023-10-01-photoguard/","summary":"@InProceedings{pmlr-v202-salman23a, title = {Raising the Cost of Malicious AI-Powered Image Editing}, author = {Salman, Hadi and Khaddaj, Alaa and Leclerc, Guillaume and Ilyas, Andrew and Madry, Aleksander}, booktitle = {Proceedings of the 40th International Conference on Machine Learning}, pages = {29894--29918}, year = {2023}, editor = {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan}, volume = {202}, series = {Proceedings of","title":"PhotoGuard"},{"content":" @misc{liang2023adversarial, title={Adversarial Example Does Good: Preventing Painting Imitation from Diffusion Models via Adversarial Examples}, author={Chumeng Liang and Xiaoyu Wu and Yang Hua and Jiaru Zhang and Yiming Xue and Tao Song and Zhengui Xue and Ruhui Ma and Haibing Guan}, year={2023}, eprint={2302.04578}, archivePrefix={arXiv}, primaryClass={cs.CV} } Project\nPaper\nCode\nVideo\nモンテカルロ法を用いて生成過程の潜在変数をおかしくするAEsを生成する手法を提案。PhotoGuardは実写画像がターゲットだったが、AdvDMはアートがターゲット。\n課題 AEsで拡散モデルを攻撃しようにも、分類モデルと比べて難しい 最適化のフローが変分境界を通して間接的に行われる=AEsが直接適用不可能 拡散モデルに対するAEsの既存の方法がない AdvDM コンセプト図。(1) 特徴抽出をOODにミスリード。(2) 摂動を付加して悪品質になるように最適化。 特徴抽出をOODにするノイズが生成されるように学習。その損失関数にモンテカルロ法を使う。\n前提 実分布を $ q(x) $ 生成分布を $ p(x) $ すると摂動 $ \\delta $ は次式で求められる。\nしかし、 $ q(x) $ は未知なので、モンテカルロ法を使って近似する。$ p_\\theta (x) $を使って、$ p_\\theta (x+\\delta) $を近似する。\n各時点の実分布の事後分布 $ q\\left(x_{1: T}^{\\prime} \\mid x_0^{\\prime}\\right) $は画像 $ x_0 $と独立な固定パラメータのガウス分布なので、生成分布 $ p_\\theta (x\\prime_(0:T)) $ は$ q\\left(x_{1: T}^{\\prime} \\mid x_0^{\\prime}\\right) $で正則化できる。\n最適化 $E_{x_{1: T} \\sim u\\left(x_{1: T}\\right)} \\mathcal{L}_{D M}(\\theta)$は期待値の損失なので、普通のAEsと違い勾配がわからない。そこで、モンテカルロ法を使って勾配を推定する。\n敵対的な生成分布 $ u (x\\prime_(1:T)) $ から $ x\\prime_(1:T) $ をサンプリングして、$ L_{D M}(\\theta) $ の勾配を推定する。\nこの推定された勾配を使ってFGSMを行う。\n異なる潜在変数になる各サンプルをイテレーション。\nAdvDMの最適化（生成）フロー 評価 Note 入力画像を使わずに、完全なガウスノイズから生成した画像は評価対象外\n＝コピーライトの心配なし 特徴抽出された特徴がOODになっていることを評価 画像から実際に抽出される条件 $c_g$のほうが無条件でサンプリングされる $c$よりも画像との類似性が高いはず\nAdvの条件cがOODになっていることの評価。$D$にはFIDやPrecision(prec., Kynkaanniemi, 2019) が利用される CFGっぽい\nアートトレースが危惧されるシナリオ Text InversionベースのT2I Text Inversionベースのスタイル変換 I2Iの変換 結果 Text-InversionベースのT2I クリーン画像とAEsでスタイル変換したときの結果。（上段）クリーン画像（下段）AdvDMで作ったAEs。注：Strengthはスタイル変換の強さパラメータ。 スタイルを強く転送しようとすると、崩壊している感じがする。\n異なるサンプリングステップを変えてAdvDMした結果。ステップを増やすほどクオリティが低いものが生成できる。 加える摂動の大きさによる攻撃効果。 条件付きT2I AEsに対する防御策をAdvDMで試した結果。一定の効果がある。 次に読む論文 Mist: Towards Improved Adversarial Examples for Diffusion Models ","permalink":"https://takumi-iida.com/posts/2023-09-27-advdm/","summary":"@misc{liang2023adversarial, title={Adversarial Example Does Good: Preventing Painting Imitation from Diffusion Models via Adversarial Examples}, author={Chumeng Liang and Xiaoyu Wu and Yang Hua and Jiaru Zhang and Yiming Xue and Tao Song and Zhengui Xue and Ruhui Ma and Haibing Guan}, year={2023}, eprint={2302.04578}, archivePrefix={arXiv}, primaryClass={cs.CV} } Project Paper Code Video モンテカルロ法を用いて生成過程の潜在変数を","title":"AdvDM"},{"content":" @inproceedings {291164, author = {Shawn Shan and Jenna Cryan and Emily Wenger and Haitao Zheng and Rana Hanocka and Ben Y. Zhao}, title = {Glaze: Protecting Artists from Style Mimicry by Text-to-Image Models}, booktitle = {32nd USENIX Security Symposium (USENIX Security 23)}, year = {2023}, isbn = {978-1-939133-37-3}, address = {Anaheim, CA}, pages = {2187--2204}, url = {https://www.usenix.org/conference/usenixsecurity23/presentation/shan}, publisher = {USENIX Association}, month = {Aug} } Project\nPaper\nアーティストのスタイルを模倣するテキストから画像への変換モデルの攻撃を防ぐツールGlazeを提案した。コンテンツを維持しつつ、スタイルは別物になるような摂動を加える。\n（Victimではない）ターゲットのスタイルに近づく特徴量シフトを生じさせ、スタイル学習に失敗するような摂動（\u0026ldquo;Style Cloak\u0026rdquo;）を加えることで、攻撃を防ぐ。 アーティストコミュニティと連携して、1156人の参加者を使って定性的な評価を行った。 93%の誤生成率、92%は元の画像スタイルを維持。 この論文では、手法の他にもアーティストが生成画像に対してどう思っているのかを記事を参照しながら紹介し、実際にトレースされた事例も挙げている。\nThreat Model アーティスト\nモデルに模倣されることを防ぎつつ作品を共有したい 知覚不能な摂動を共有前に自分の作品に加えて自己防衛したい ノーパソみたいな貧弱な計算資源しかないかも 模倣者（攻撃者）\nVictim Styleの高品質な画像を生成したい 重みにアクセス可能 ターゲットのアーティストの画像を数枚入手可能 十分な計算資源を持っている 攻撃シナリオ Glazeは学習されても模倣されないような画像を生成することを目指す。\n模倣防衛シナリオ 先行研究 顔画像保護 \u0026ldquo;image cloaking\u0026rdquo;: ユーザ画像の特徴表現から劇的に変わるような摂動を加える ただし、text-to-imageのような大規模な特徴空間では機能しない 画像生成するには多くの属性情報が特徴量に含まれるため、その空間上で同様の摂動を作るのは難しい（生成モデルに対する摂動が難しいという研究がある） PhotoGuard 生成モデルに対するAEsを行い許可のない画像編集を防ぐ方法 画像の情報をすべて最小化してしまうため、模倣は防げない（模倣は編集ではなく、FTによる学習が行われるため？） Glaze Glazeの全体像 Style Transferを使って、Victimのオリジナル画像を様々なスタイルに変換\nStyle-Transferの結果 シフト先のスタイル（ターゲット）の画像は、各アーティストとVictimの特徴中心を特徴抽出気（$\\Phi$）を使って計算。距離が50-75パーセンタイルにあるものをターゲットスタイルとして候補に選択する。 このスタイル変換した画像を使って摂動をガイドする\n$$ \\begin{gathered} \\min _{\\delta_x} \\operatorname{Dist}\\left(\\Phi\\left(x+\\delta_x\\right), \\Phi(\\Omega(x, T))\\right) \\ \\text { subject to }\\left|\\delta_x\\right|\u0026lt;p \\end{gathered} $$ ここで、$\\Omega (x, T)$は画像xを既存のスタイル変換モデル$\\Omega$でTのスタイルに変換した画像、$\\delta_x$は摂動を表す。 最終的な損失関数は以下のようになる。 $$ \\min _{\\delta_x}\\left|\\Phi(\\Omega(x, T)), \\Phi\\left(x+\\delta_x\\right)\\right|_2^2+\\alpha \\cdot \\max \\left(\\operatorname{LPIPS}\\left(\\delta_x\\right)-p, 0\\right) $$\n結果 アーティストとCLIP Scoreによる防御率評価 定性評価 摂動量による防御率変化 摂動量の違いによる各指標 どれくらい摂動が加わっていたら投稿したいか （左）Victimと模倣者で別々の特徴抽出器$\\Phi$を使った場合。（右）すでにネット上に画像をアップロードしているアーティストの場合、全体の25%くらいしの作品を保護できていれば87.2%くらいの全体防御率になる。 ガウスノイズを加えてプロテクトを解除しようとしてもあまり効果がない。JPEG圧縮も同様。 補足 現実世界でのトレース事例 Hollie Mengertの作品がReddit上のモデルでトレースされた。 BAIO, A. Invasive Diffusion: How one unwilling illustrator found herself turned into an AI model, 2022\n漫画家Sarah Andersenが自身の作品がトレースできると報告 The Alt-Right Manipulated My Comic. Then A.I. Claimed It. The New York Times, 2022 その関連 MURPHY, B. P. Is Lensa AI Stealing From Human Art? An Expert Explains The Controversy. ScienceAlert, 2022. YANG, S. Why Artists are Fed Up with AI Art. Fayden Art, Dec. 2022. いくつかの会社はそれをサービスとして展開 SCENARIO.GG. AI-generated game assets, 2022 数枚の画像をアップロードすると、それに似たスタイルの画像を生成してくれる。 CivitAIはトレースした作品/モデルをシェアするプラットフォーム 感想 スタイルだけでOKなのか。コンテンツはトレースされそう。 AEsの傾向はそのまま維持されてそう ","permalink":"https://takumi-iida.com/posts/2023-09-23-glaze/","summary":"@inproceedings {291164, author = {Shawn Shan and Jenna Cryan and Emily Wenger and Haitao Zheng and Rana Hanocka and Ben Y. Zhao}, title = {Glaze: Protecting Artists from Style Mimicry by Text-to-Image Models}, booktitle = {32nd USENIX Security Symposium (USENIX Security 23)}, year = {2023}, isbn = {978-1-939133-37-3}, address = {Anaheim, CA}, pages = {2187--2204}, url = {https://www.usenix.org/conference/usenixsecurity23/presentation/shan}, publisher = {USENIX Association}, month = {Aug} } Project Paper アー","title":"Glaze"},{"content":"動機とか 以前はJekyllを使って自前サイトを作っていたのですが、なんか微妙に感じていました。\nHugoもちょっと触ってみたいと思ったので、触ってみたら結構簡単でかつ好みのテーマがあったので、乗り換えてみようかなと感じました。\nもう見なくなるサイトなので、写真だけ貼って供養します。\n論文を時系列に並べて関係性を広い意味で整理しようと Archive を投稿順に並べるところに当時はこだわりましたが、あんまり使わなかったですね。 あと、あんまり長い記事は書かなかったので、記事のスクロールに合わせて「アウトライン」のハイライトが遷移する機能も必要なかったですね\u0026hellip;。\n旧ページのギャラリー トップページ\n記事ページ アーカイブページ\nわりかし苦労したやつ。レイアウトにこだわった。改善余地はあるが、当時の自分としては満足した。\n移行に当たってのメモ 環境 Git Go Hugo-extended\nバイナリファイルをダウンロード PaperMod hugo-embed-pdf Hugo Icons Module 起動コマンド忘れそうなのでメモ。\n# 起動 $ hugo server -D Custom ShortCode お知らせブロック Info A notice disclaimer Note A notice disclaimer Tip A notice disclaimer Warning A notice disclaimer BibTexの引用 @misc{liang2023adversarial, title={Adversarial Example Does Good: Preventing Painting Imitation from Diffusion Models via Adversarial Examples}, author={Chumeng Liang and Xiaoyu Wu and Yang Hua and Jiaru Zhang and Yiming Xue and Tao Song and Zhengui Xue and Ruhui Ma and Haibing Guan}, year={2023}, eprint={2302.04578}, archivePrefix={arXiv}, primaryClass={cs.CV} } いろんなリンク Project\nPaper\nCode\nVideo\nキャプション付きで画像をタイル状に並べる 画像1 画像2 画像3 テーマのカスタマイズ ホーム Biography: layouts/biography/single.html 記事一覧: layouts/_default/list.html ソート: laytout/_default/sorted-by-pubdate.html 画像添付: layouts/_default/_markup/render-link.html [ココ](link)がキャプションになるようにした 数式レンダラ: layouts/partial/math.html BibTexの引用: layouts/partial/cite.html, static/js/cite.js 出版年月を数値表記にするためJSを使用 ","permalink":"https://takumi-iida.com/posts/2023-09-18-introduction/","summary":"動機とか 以前はJekyllを使って自前サイトを作っていたのですが、なんか微妙に感じていました。 Hugoもちょっと触ってみたいと思ったので、触","title":"移行しました"},{"content":" @InProceedings{Erhan_2014_CVPR, author = {Erhan, Dumitru and Szegedy, Christian and Toshev, Alexander and Anguelov, Dragomir}, title = {Scalable Object Detection using Deep Neural Networks}, booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, month = {June}, year = {2014} } Paper\nVideo\nMultiBoxは、画像中に複数インスタンスが会った場合でも、Confidence付きのbboxを複数出力可能なモデルです。これも重要論文に数えられるんですが、解説がなかなかありません。おそらく単純な方法だからだとおもうんですが、一応読んでみました。軽くさらっていきます。 概要 一枚の画像中で複数の同一インスタンスがあった場合に、複数のbboxをconfidence付きで出力できるようにした論文です。\n従来手法だと、一枚画像を入力して、各クラスに対して単一のbboxを推論するネットワークしかなかったらしいんですが、MultiBoxでは、画像中に複数の同一インスタンスがあっても、一発で複数の領域提案が行えるようにしています。 Multiboxはクラスに依らない領域提案を学習しているので、高い汎化能力があるそうです。\nMultibox 論文を読んだ感じだと、次図のアーキテクチャになると思います。共通のAlexNetのバックボーンがあり、そこからbbox回帰と物体らしさ(Objectness)のconfidenceを出力する2つのブランチがあります。\nMultiboxは、事前に一枚の画像に含まれる最大の物体数Kを決めておきます。そして、K個のbboxの位置とconfidenceを推定した後に、NMSやConfidenceを使って、不要なbboxの廃棄(supression)を行います。論文中では\\( K = 100, 200 \\)を使用しています。\nほとんど解説が終わってしまったんですが、一応各ブランチでの損失関数などを解説します。\nbbox回帰ブランチ bbox回帰ブランチでは、K個あるbbox候補のGT座標への回帰を行います。出力は、GT bboxの左上と右下のxy座標の4点です。\n損失関数は次式です。ここで、\\( x_{ij} \\)は、推論bbox\\( i \\)がGT bbox \\( j \\)と対応しているとき1になりそれ以外は0になるバイナリ値です。それ以外は普通にL2ロスで最適化しています。筆者は、この\\( x_{ij} \\)の理解が怪しいです。\n$$ F_{\\text {match }}(x, l)=\\frac{1}{2} \\sum_{i, j} x_{i j}\\left|l_{i}-g_{j}\\right|_{2}^{2} $$\nConfidence推定ブランチ Confidenceブランチでは、各bboxに対応したConfidence値を推定します。損失関数は、次式です。bbox回帰ブランチに入ってくる\\( x_{ij} \\)を除くと、バイナリクロスエントロピーロスと同じ形をしています。物体かどうかの判定をしています。推論bbox\\( i \\)のGT bbox \\( j \\)へのConfidenceが上昇すると損失が下がる構造になっています。\n$$ F_{\\mathrm{conf}}(x, c)=-\\sum_{i, j} x_{i j} \\log \\left(c_{i}\\right)-\\sum_{i}\\left(1-\\sum_{j} x_{i j}\\right) \\log \\left(1-c_{i}\\right) $$\n最後に上記２つのブランチの損失をマージした損失関数（次式）をまとめて最適化していきます。詳しい学習方法は割愛します。\n$$ F(x, l, c)=\\alpha F_{\\text {match }}(x, l)+F_{\\text {conf }}(x, c) $$\n","permalink":"https://takumi-iida.com/posts/2021-07-25-multibox/","summary":"@InProceedings{Erhan_2014_CVPR, author = {Erhan, Dumitru and Szegedy, Christian and Toshev, Alexander and Anguelov, Dragomir}, title = {Scalable Object Detection using Deep Neural Networks}, booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, month = {June}, year = {2014} } Paper Video MultiBoxは、画像中に複数インスタンスが会","title":"MultiBox"},{"content":" @misc{sermanet2014overfeat, title={OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks}, author={Pierre Sermanet and David Eigen and Xiang Zhang and Michael Mathieu and Rob Fergus and Yann LeCun}, year={2014}, eprint={1312.6229}, archivePrefix={arXiv}, primaryClass={cs.CV} } Paper\nCode\nVideo\n従来手法とCNNを組み合わせた初の論文です。R-CNNと似たような位置づけだが、R-CNNは2ステージ検出器、OverFeatは1ステージ検出器の元祖といえると思います。\nOverFeatは、CNN for Object Detection最初期の論文ですが、R-CNNが話題に出ることが多く、重要な位置づけの論文の割に日本語の解説ページがあまりないことに気づきました。ほとんど英語の解説記事や解説動画の焼き増し的な記事になると思いますが、日本語解説を用意することに意義があると思いました。\n概要 OverFeatは、物体検出タスクを物体認識タスクと位置推定タスクを統合して実現した、スライディングウィンドウ方式の手法です。\n位置推定タスクと物体検出タスクの区別がしっくりと来ませんが、\n位置推定タスクは、画像中に何か物体があることは保証されていて、どこにその物体があるのかを推定するタスク 物体検出タスクは、画像中に物体があることは保証されておらず、何もなければ結果を出力しないような処理が入るもの だと理解しました。\nこの論文では、これらを別々のタスクとしてまずは扱い、それらを統合するようなフレームワークになっています。\nOverFeatのフレームワーク 論文本来のフローとかなり流れが違いますが、どうにもわかりにくいので、Cogneethiの動画をベースに解説していきます。もち違っている点があれば、ツイッターとかでご指摘ください。\nOverFeatのクラス分類・位置推定のブランチは、画像全体のCNNの特徴マップを直接受け取って、最後にそれら２つを統合する処理が入っています。\n全体のざっくりした処理フローは、次の図のようになっており、いわゆる領域提案は行わず、1ステージの構造になっています。CNNの部分はAlexNetベースのネットワークになっており、ImageNetで事前に学習済みのものを利用します。\nそれぞれのモジュールについて解説していきます。\n分類ブランチ 分類ブランチの中身はFully Convolutional Network(FCN)で構成されており、Conv層のみです。そのため、任意のサイズの入力を受け取ることができます。\nOverFeatは、複数スケールの入力を受け取り、異なるサイズの特徴マップを出力します。すると、Conv層には、受容野があるので、次図の5x5の畳み込みが分類ブランチの一回目で行われれば、特徴の由来の対応関係がグレーのセルのようになります。\nすると、この特徴マップのセルの中に物体があるかどうかを判定できれば、入力画像で粗いスライディングウィンドウをやったのと同じような扱いにすることができます。つまり、スライディングウィンドウのように実際に入力画像を切り出すのではなく、画像全体を入力して、特徴空間で切り出しをしているようなイメージです。空間的な探索空間が大幅に減るので、かなり高速化されます。281x317サイズで画像を入力したときの、2x3の出力で左上のセルに物体があると判定されれば、画像の左上の方にも物体があると解釈することができます。\n回帰ブランチ 分類ブランチで、大まかにどこあたりに物体がありそうかというのはわかりますが、かなり解像度があらいので、入力画像上でどの位置にあるのかを求める必要があり、回帰ブランチではそれを行います。\n回帰ブランチの構造は、分類ブランチとほぼ同様です。違うのは、最後の出力サイズくらいです。bboxの座標を表現する4点(x1, y1, x2, y2)を推論するために、マップが4倍になっています。\n統合 最後に、分類ブランチと回帰ブランチの出力結果の統合を行います。\n現在よく行われているNMSではなく、貪欲な方法で統合を行っています。\n統合の手順は以下の流れです。\n各スケール\\(s \\in 1\u0026hellip;6 \\)でtop-kのクラス集合を\\(C_s \\)に割当てる。top-kは、各スケールの空間位置を跨いで、計算します。 スケール\\(s \\)で、全ての空間位置に対して、クラス集合\\(C_s \\)に対応するクラスのbbox出力を\\(B_s \\)に割り当てる。 各スケールのbbox集合\\(B_s \\)を一つの集合にする。(\\(B \\leftarrow \\bigcup_{s} B_{s} \\)) 各bbox候補をマージしていく。bbox同士のマッチスコアが小さい順からマージして、しきい値を超えると、マージを止める。 4. のbbox同士のマッチスコアは、bboxの中心の座標同士の距離を全て計算します。計算した結果の昇順の組み合わせで、マージを実行していきます。マージ後のbboxは、それぞれのbboxの平均になります。\n感想 言わずもがな、1ステージ検出器の開拓手法として非常に重要な論文ですね。 複数入力必要なのが難点ですが、フォワード計算はかなり高速なのが嬉しい。 一方で、検出性能がR-CNNよりも劣るのが難点。 まあ、スキップコネクションなしで、特徴マップ上でのObjectnessを計算しなきゃいけないのと、 分類ブランチの学習がマルチスケールに対応できていないような気がするので、推論時にギャップが生じている気がする。\n参考 C 5.6 Overfeat , Network Design Important-Dont skip, CNN, Object Detection , EvODN OverFeat , Lecture 38 (Part 1) , Applied Deep Learning Overfeat Review(1312.6229) ","permalink":"https://takumi-iida.com/posts/2021-07-24-overfeat/","summary":"@misc{sermanet2014overfeat, title={OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks}, author={Pierre Sermanet and David Eigen and Xiang Zhang and Michael Mathieu and Rob Fergus and Yann LeCun}, year={2014}, eprint={1312.6229}, archivePrefix={arXiv}, primaryClass={cs.CV} } Paper Code Video 従来手法とCNNを組み合わせた初の論文です。R-CNNと似たような位置","title":"OverFeat"},{"content":" @InProceedings{Girshick_2014_CVPR, author = {Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra}, title = {Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation}, booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, month = {June}, year = {2014} } Paper\nCode\nCNNと領域提案手法を統合した先駆け的論文。従来のSoTAから大きく性能が向上したが、一方で多段階のパイプラインに難ありで、実行時間も長いのが難点。\n概要 R-CNNは、CNNを物体検出に取り込んだ先駆け論文として広く知られているモデルです。 従来の方法では、スライディングウィンドウで、超大な数の領域をあらかた提案してCNNに突っ込んで学習するような方法が考えられてきました。しかし、それだと探索範囲が広すぎるため、より効率的に物体らしい領域を提案する必要がありました。R-CNNはSelective Searchを利用して、効率的に領域提案を行い、CNNが実行可能な程度に効率化を図ったモデルです。\nR-CNNの実行手順は以下の流れになります。\nSelective Searchを使った領域提案 で提案された領域を入力画像から切り出し 切り出した領域を一定サイズにワープ ワープした画像をCNNに入力 複数のSVMでクラス分類、bbox回帰を行う。 もう少し詳しく解説します。\nSelective Searchによる領域提案 まず、1. でR-CNNではSelective Searchを利用した領域提案を行っています。Selective Searchでは、次の手順で領域提案を行います。解説はかなりの部分をしこあんさんのブログをベースとしています。\nFelzenszwalb法による画像のセグメンテーションを行い、各セグメントを一つの領域候補とする。 各領域候補の色とテクスチャのヒストグラム特徴量を作成する。 各領域同士で重なりのあるセグメントの組み合わせをNeighborとして全列挙する。 の特徴量を使い、3. の全ての組み合わせから領域候補の類似度を計算。類似度計算には、下記の4項目を利用する。 色（2. で計算済みのRGBのヒストグラム特徴量） テクスチャ(2. で計算済みのLocal Binary Pattern(LBP)のヒストグラム特徴量) サイズ オーバーラップ度合い での類似度が高いもの同士をマージする。（Hierarical Search） LBPは、エッジの情報がロバストに抽出できる手法として知られています。グレースケール化後、注目ピクセルの周囲に対して自身よりも値が、大きい（\u0026ndash;\u0026gt; 1）小さい（\u0026ndash;\u0026gt; 0）かのバイナリ値を作成した後に、真上から反時計回りに並べた2進数を出力します。\nこちらの記事より作成。\n試しにsk-imageの関数でLBPをした結果が次の図です。エッジが保たれ、テクスチャが抽出されていることが確認できます。\nこのLBP特徴画像とRGBの各チャネルに対して2. でヒストグラム特徴量を求めます。そして、隣接するセグメント同士でヒストグラム特徴量同士の類似度を計算して、再帰的に類似度が高いセグメントを結合していきます。セグメントと書いていますが、実際にはセグメントを囲む提案領域の矩形をマージしています。 これにより、Selective Searchによる領域提案を行います。\nテスト時には2000個程度の領域が提案されます。\n特徴抽出/クラス分類/ bbox回帰 Selective Searchによって提案された領域は元画像からクロップされて一定サイズにワープされます。論文中では、特徴抽出にはAlexNetが使われているので、227x227サイズのRGB画像にリサイズされて、4096次元の特徴ベクトルが各提案領域について出力されます。具体的には5つのConv層と2つのFC層を通過します。FC層が挟まっているのもあって、固定サイズにリサイズしなければいけません。\nこうして提案領域のCNN特徴量は、複数のSVMを使ってクラススコアを計算し、各クラスで独立にNMSさせて結果を出力します。SVMは一緒に学習するのではなく、各クラスで事前学習したものを利用します。\nbbox回帰では、線形回帰モデルを使っている。DPMという手法に影響されたらしい。\n感想 Faster R-CNNよりは需要低いけど、古典的アルゴからCNNベースへの橋渡し的な位置づけの論文だと思うので、抑えておくのは重要だと感じた。 複数SVMをクラス分類に利用していて、かつ事前学習しなければいけないのはかなり面倒だし計算コストも高いと感じた。 ","permalink":"https://takumi-iida.com/posts/2021-07-24-r-cnn/","summary":"@InProceedings{Girshick_2014_CVPR, author = {Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra}, title = {Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation}, booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, month = {June}, year = {2014} } Paper Code CNNと領域提案手法を統合した先駆け的論文","title":"R-CNN"},{"content":"","permalink":"https://takumi-iida.com/pubdate/","summary":"","title":"記事で紹介した論文の出版年"},{"content":"","permalink":"https://takumi-iida.com/aboutme/","summary":"","title":"飯田啄巳"}]